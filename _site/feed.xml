<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2019-12-15T08:48:16-06:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Akhila Yaragoppa</title><subtitle>Data analysis portfolio</subtitle><author><name>Akhila Yaragoppa</name><email>akhila171@gmail.com</email></author><entry><title type="html">Titanic - An exploratory analysis</title><link href="http://localhost:4000/titanic/" rel="alternate" type="text/html" title="Titanic - An exploratory analysis" /><published>2019-12-15T00:00:00-06:00</published><updated>2019-12-15T00:00:00-06:00</updated><id>http://localhost:4000/titanic</id><content type="html" xml:base="http://localhost:4000/titanic/">&lt;p&gt;This is an exploratory analysis of the Titanic survivors data from Kaggle.&lt;/p&gt;

&lt;p&gt;On April 15, 1912, during her maiden voyage, the widely considered “unsinkable” RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren’t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.&lt;/p&gt;

&lt;p&gt;While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.&lt;/p&gt;

&lt;p&gt;In this article, I will attempt to identify what category of passengers most likely survived.&lt;/p&gt;

&lt;p&gt;The features that can be examined for each passenger are:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Age&lt;/li&gt;
  &lt;li&gt;Sex&lt;/li&gt;
  &lt;li&gt;Ticket class&lt;/li&gt;
  &lt;li&gt;Number of siblings / spouses aboard the Titanic&lt;/li&gt;
  &lt;li&gt;Number of parents / children aboard the Titanic&lt;/li&gt;
  &lt;li&gt;Passenger fare&lt;/li&gt;
  &lt;li&gt;Port of Embarkation&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;age&quot;&gt;Age&lt;/h2&gt;
&lt;p&gt;Let’s first have a look at the number of survivors and non-survivors among passengers of different ages.
&lt;img src=&quot;/images/titanic/age.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The following observations can be made from the graph above:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Most passengers on the ship are in the age range 15 to 50.&lt;/li&gt;
  &lt;li&gt;A high number of non-survivors exist in the age range 20-30.&lt;/li&gt;
  &lt;li&gt;There are also a good number of survivors in the same age range.&lt;/li&gt;
  &lt;li&gt;Of the 8 passengers above the age of 65, only 1 survived.&lt;br /&gt;
From the above observations, I can assume that ‘age’ can be used as a decision factor for evaluating passenger survival.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;sex&quot;&gt;Sex&lt;/h2&gt;
&lt;p&gt;Let’s now see if there is a bias in number of survivors with respect to sex of the passenger.
&lt;img src=&quot;/images/titanic/male-female-survivors.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Observations from this graph:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Approximately 70% of the female passengers survived.&lt;/li&gt;
  &lt;li&gt;Only 20% of the male passengers survived.
Therefore, we can use this feature to significantly improve survival prediction of a passenger.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;ticket-class&quot;&gt;Ticket Class&lt;/h2&gt;
&lt;p&gt;Next, let’s see how the ticket class affected the survival chances of a passenger:&lt;br /&gt;
&lt;img src=&quot;/images/titanic/Pclass.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Observations from this graph:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;More than 50% of passengers with Class 1 tickets survived.&lt;/li&gt;
  &lt;li&gt;Around 50% of passengers with Class 2 tickets survived.&lt;/li&gt;
  &lt;li&gt;Only around 30% of passengers with Class 3 tickets survived.
This tells us that somehow even during a life/death situation like the sinking of the Titanic, people with better tickets had a higher chance of survival.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;ticket-fare&quot;&gt;Ticket Fare&lt;/h2&gt;
&lt;p&gt;Another graph on similar lines as the ticket class, is the graph of ticket fare.&lt;br /&gt;
&lt;img src=&quot;/images/titanic/fare.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;
From the above graph, we can see that only 4 people that did not survive, paid a ticket fare above 100. Also, from the graph of non-survivors, we see that a very high number of non-survivors are concentrated around low ticket prices.&lt;br /&gt;
We see that this graph affects the survival in the same way that the ticket class did. It is possible that most information given by this variable is redundant. If this were a huge dataset with many features, we could possibly ignore this feature, or generate a new feature from the ‘ticket class’ and the ‘ticket fare’.&lt;/p&gt;

&lt;h2 id=&quot;port-of-embarkation&quot;&gt;Port of embarkation&lt;/h2&gt;

&lt;h3 id=&quot;titanic-sailing-route-map&quot;&gt;Titanic sailing route map&lt;/h3&gt;
&lt;p&gt;Titanic first arrived at Southampton, UK.&lt;br /&gt;
6 days later, Titanic arrived at Cherbourg, France.&lt;br /&gt;
Titanic arrived in Queenstown, Ireland a day later.&lt;br /&gt;
3 days later Titanic hits the iceberg.&lt;/p&gt;

&lt;p&gt;Let’s look at a graph of the survival variation based on the ‘port of embarkation’.&lt;br /&gt;
&lt;img src=&quot;/images/titanic/embarkation.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Observations:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;A huge number of people boarded from Southampton. This was probably because it was the first port where people boarded.&lt;/li&gt;
  &lt;li&gt;Only about 30% of the passengers who boarded in Southampton survived.&lt;/li&gt;
  &lt;li&gt;A small number of people boarded in Queenstown, and about 40% of them survived.&lt;/li&gt;
  &lt;li&gt;In Cherbourg, more than 50% of the passengers survived.  This was probably because most passengers who boarded in Cherbourg had class 1 tickets.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;This analysis gives us a fair idea of what factors most determined the survival of a passenger aboard the titanic. These features can be used to build machine learning models to predict the survival of a passenger. A preliminary knowledge of the data sets a much required context to tweak the features as necessary for building efficient machine learning models.&lt;/p&gt;

&lt;h2 id=&quot;code&quot;&gt;Code&lt;/h2&gt;
&lt;p&gt;The above analysis was carried out using a jupyter notebook. The notebook can be found &lt;a href=&quot;https://github.com/akhilayaragoppa/akhilayaragoppa.github.io/blob/master/source_code/Titanic-Data-Visualization.ipynb&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;</content><author><name>Akhila Yaragoppa</name><email>akhila171@gmail.com</email></author><category term="Data Visualization" /><summary type="html">This is an exploratory analysis of the Titanic survivors data from Kaggle.</summary></entry><entry><title type="html">An Analysis of Course Feedback Forms</title><link href="http://localhost:4000/feedback-forms/" rel="alternate" type="text/html" title="An Analysis of Course Feedback Forms" /><published>2015-11-30T00:00:00-06:00</published><updated>2015-11-30T00:00:00-06:00</updated><id>http://localhost:4000/feedback-forms</id><content type="html" xml:base="http://localhost:4000/feedback-forms/">&lt;p&gt;An analysis of course feedback forms to predict the instructor, students attendance and number of repeats for that course.&lt;/p&gt;

&lt;h2 id=&quot;data&quot;&gt;Data&lt;/h2&gt;

&lt;p&gt;The data is a collection of course feedback forms from a group of 5820 students, along with the attendance, number of repeats, and the difficulty level of the course for which the feedback has been submitted.&lt;/p&gt;

&lt;p&gt;The data has four sets of classes that it can be classified into –&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Instructor (1, 2, 3)&lt;/li&gt;
  &lt;li&gt;No. of repeats (1, 2, 3)&lt;/li&gt;
  &lt;li&gt;Attendance (0, 1, 2, 3, 4)&lt;/li&gt;
  &lt;li&gt;Difficulty (1, 2, 3, 4, 5)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are 28 features in all, each one being a rating on a scale of 1 to 5 of some aspect of the course or the instructor.&lt;/p&gt;

&lt;p&gt;I tried feeding the features into a neural network, and based on the feedback of a random student I tried to predict –&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;the number of repeats&lt;/li&gt;
  &lt;li&gt;the attendance of a student&lt;/li&gt;
  &lt;li&gt;the instructor&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;neural-network&quot;&gt;Neural network&lt;/h2&gt;

&lt;p&gt;I used a two-layer feed-forward network, with sigmoid hidden and softmax output neurons. This can classify vectors arbitrarily well, given that there are enough neurons in its hidden layer.&lt;/p&gt;

&lt;h2 id=&quot;predicting-the-number-of-repeats&quot;&gt;Predicting the number of repeats&lt;/h2&gt;
&lt;p&gt;I first trained the network to predict the no. of repeats. I got a very high accuracy of ~80%. On closer observation, I found that due to the highly biased distribution of the data points in one class (0 repeats), the network predicted almost every point as belonging to that class.
&lt;img src=&quot;/images/feedback/bar-1.PNG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/feedback/mat-1.PNG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;predicting-the-attendance&quot;&gt;Predicting the attendance&lt;/h2&gt;
&lt;p&gt;I then tried to train the network to predict the attendance. The attendance histogram had a better distribution than the previous case. But, I only managed to get an accuracy of about 37% with the validation data. This probably happened because the number of possible classes increased to 5.
&lt;img src=&quot;/images/feedback/bar-2.PNG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/feedback/mat-2.PNG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;predicting-the-instructor&quot;&gt;Predicting the instructor&lt;/h2&gt;
&lt;p&gt;Finally, I trained the network to predict the instructor for a given set of feedback values. There were only 3 instructors, and the distribution looked like this.
&lt;img src=&quot;/images/feedback/bar-3.PNG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/feedback/mat-3.PNG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;varying-the-number-of-neurons-to-increase-prediction-accuracy&quot;&gt;Varying the number of neurons to increase prediction accuracy&lt;/h3&gt;
&lt;p&gt;With 10 neurons in the hidden layer, I obtained an accuracy of 59% for the validation data. Similarly, for 5 and 20 neurons in the hidden layer, I got accuracies of 62.4% and 62.8% respectively.&lt;/p&gt;

&lt;h3 id=&quot;varying-the-number-of-iterations-for-better-performance&quot;&gt;Varying the number of iterations for better performance&lt;/h3&gt;
&lt;p&gt;At the 37th iteration, I obtained the best validation performance.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/feedback/validation.PNG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;</content><author><name>Akhila Yaragoppa</name><email>akhila171@gmail.com</email></author><category term="Machine Learning" /><category term="Neural Networks" /><summary type="html">An analysis of course feedback forms to predict the instructor, students attendance and number of repeats for that course.</summary></entry><entry><title type="html">Classification of Songs - Million Song Dataset</title><link href="http://localhost:4000/million-song-dataset/" rel="alternate" type="text/html" title="Classification of Songs - Million Song Dataset" /><published>2015-09-15T00:00:00-05:00</published><updated>2015-09-15T00:00:00-05:00</updated><id>http://localhost:4000/million-song-dataset</id><content type="html" xml:base="http://localhost:4000/million-song-dataset/">&lt;p&gt;An analysis of the properties of a song to identify the decade to which the song belonged.&lt;/p&gt;

&lt;h2 id=&quot;dataset&quot;&gt;Dataset&lt;/h2&gt;

&lt;p&gt;The &lt;strong&gt;Million Song Dataset&lt;/strong&gt; is a freely-available collection of audio features and metadata for a million contemporary popular music tracks. This project uses a &lt;a href=&quot;http://millionsongdataset.com/pages/getting-dataset/#subset&quot;&gt;subset&lt;/a&gt; of this dataset. The data contains 515,345 songs, each described by 90 features, and the year in which the song was released. The years range from 1922 to 2011.&lt;/p&gt;

&lt;h2 id=&quot;exploratory-data-analysis&quot;&gt;Exploratory Data Analysis&lt;/h2&gt;

&lt;p&gt;For simplicity, the number of classes is reduced from 90 to 9 by assigning a decade to each song, instead of year.
A histogram of the count of songs in each decade showed an unevenly distributed data:
&lt;img src=&quot;http://localhost:4000/images/hist-unmodified.PNG&quot; alt=&quot;Hist-unmodified&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In an attempt to obtain a more evenly distributed data, the width of the classes was altered, thus resulting in a distribution as shown below:
&lt;img src=&quot;/images/hist-modified.PNG&quot; alt=&quot;hist-modified&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;choosing-features&quot;&gt;Choosing Features&lt;/h2&gt;
&lt;p&gt;A feature can be chosen for classification, if it peaks at different places for different classes. For example, I chose ‘Timbre average 1’ as one of the classifiers as it peaked at various regions for Class 1 and Class 2 as shown in the below figure:
&lt;img src=&quot;/images/hist-f1-c1-c2.PNG&quot; alt=&quot;hist-f1-c1-c2&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;principal-components&quot;&gt;Principal Components&lt;/h3&gt;
&lt;p&gt;A scatter plot of the first two principal components looked something like this:
&lt;img src=&quot;/images/scatter-pc.PNG&quot; alt=&quot;scatter-pc&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Two important observations can be made from this plot:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;There is a significant difference in the amount of data between the first few classes and the last couple of classes (it is obscured by pink).&lt;/li&gt;
  &lt;li&gt;The colors are all bundled together. Ideally, the different colors must be local to different regions on the plot, so that a more accurate decision can be made when predicting the class of some unknown data.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;bayes-classification&quot;&gt;Bayes’ Classification&lt;/h2&gt;
&lt;p&gt;The test data was first classified without reducing any dimensions (i.e., using all 90 features independently). An accuracy of 22% was obtained.&lt;/p&gt;

&lt;p&gt;The 90 features were then projected in the direction of the 2 eigenvectors corresponding to the 2 highest eigenvalues. Classifying with the help of the 2 principal components returned an accuracy of around 50%. As this result seemed odd, the procedure was rechecked.
This lead to a conclusion that the dataset has the &lt;strong&gt;curse of dimesionality&lt;/strong&gt;. The problem is that when the dimensionality increases, the volume of the space increases so fast that the available data becomes sparse. So, a huge amount of data is required to support analysis in higher dimensions.&lt;/p&gt;

&lt;p&gt;After modyfying class sizes to obtain a more evenly distributed data, the prediction accuracy was in fact lower (only about 35%). I do not have a plausible explanation for this. The decision boundary looks as below:
&lt;img src=&quot;/images/dec-bound.PNG&quot; alt=&quot;decision-boundary&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;implementation&quot;&gt;Implementation&lt;/h2&gt;
&lt;p&gt;The MATLAB code written to perform the above analysis:&lt;/p&gt;
&lt;div class=&quot;language-matlab highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;raw&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'YearPredictionMSD.txt'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;raw&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(:,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;91&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;%Separate data for train and test as given in website&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;%separate class info from data&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;trainingData&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;463715&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,:);&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;class_traingData&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;raw&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;463715&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;r_class_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ClassReduce&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;class_traingData&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;testingData&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;463716&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;515345&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,:);&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;class_testingData&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;raw&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;463716&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;515345&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;r_class_test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ClassReduce&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;class_testingData&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;%histogram of two features&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;hist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trainingData&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(:,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;hist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trainingData&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(:,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;13&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;% find 2 principle components(PC)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;coeff&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pca&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;coeff&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;coeff&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(:,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;%figure, mesh(coeff);&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;% finding data representation along PC&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trainingData&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;lowrep_trainingData&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trainingData&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trainingData&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;lowrep_trainingData&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,:)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trainingData&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,:)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;coeff&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dim_cov&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;cov&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lowrep_trainingData&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;testingData&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;lowrep_testingData&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;testingData&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;testingData&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;lowrep_testingData&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,:)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;testingData&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,:)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;coeff&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NaiveBayes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trainingData&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r_class_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;test_op&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;testingData&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_op&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r_class_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;acuracy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)/&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;model2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NaiveBayes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lowrep_trainingData&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r_class_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;test_op1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lowrep_testingData&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;result1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_op1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r_class_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;acuracy1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;result1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)/&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;result1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>Akhila Yaragoppa</name><email>akhila171@gmail.com</email></author><category term="Machine Learning" /><category term="Bayesian Classification" /><summary type="html">An analysis of the properties of a song to identify the decade to which the song belonged.</summary></entry></feed>