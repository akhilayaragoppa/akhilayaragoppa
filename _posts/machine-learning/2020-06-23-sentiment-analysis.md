---
title: "Sentiment Analysis of Amazon Product Reviews"
date: 2020-06-23
tags: [NLP]
header:
  image: "/images/ds.jpg"
  excerpt: "NLP"
---
A classification of text-based product reviews into POSITIVE and NEGATIVE.

## Data
The data is collected from reviews of various books. A review given by a single user contains some text and a rating on the scale of 1-5. Text associated with 1,2 ratings is considered *negative*, and those associated with 4,5 is considered *positive*.

## Data Processing
The text data is converted into numeric vectors so that it can be passed into some classification models. The conversion from text to numerical vectors is done using 'Bag of Words'.

## What is Bag of Words?
Bag of words simply keeps track of all the words occurring and their corresponding frequency of occurrence. Every distinct word occurring in the entire training data is assigned an index. Then a vector is formed where each entry in the vector is the number of times the word at that index occurred.

## Classification
The transformed vectors are fit using multiple classification models namely SVM, Decision Trees, Naive Bayes and Logistic Regression.

## Prediction
This trained model is able to predict the sentiment of a new unseen text.

## Code
Here is the [jupyter notebook](https://github.com/akhilayaragoppa/akhilayaragoppa.github.io/blob/master/source_code/sentiment-analysis.ipynb) where I implemented the above technique.

## Where 'Bag of Words' fails?
* When an unseen word that was not present in the training data is given to the model for prediction, it simple ignores that word. This word could potentially change the tone of the review, but is ignored if the machine has not seen it before.
* It does not understand contextual meaning. It does not take into consideration the meaning of a word in combination with the words adjoining a given tone indicating word.
  - Ex: 'This book was not good' and 'Not only was this book good, it was excellent.' These two sentences seem similar to this model.
* It cannot comprehend that two same words can have slightly differing spellings.
  - Ex: 'book' and 'books' are two different words to this algorithm.

## What is good about 'Bag of Words' then?
* The number of features generated by the model is much less compared to other state of the art techniques. This allows for use of simple classification models and quick computation.
* It is a very intuitive algorithm.
  - As the complexity of algorithms in the machine learning field is increasing everyday, it can be very important to have simpler algorithms that can provide intuitive understanding for beginners, and motivate them to contribute in making the complex algorithms more intuitive.

## What are other state of the art techniques that can solve the same problem?
* Word2Vec
  - This is an algorithm that can be used to convert words to vectors.
  - It solves most issues in bag of words technique, such as: contextual understanding, understanding unseen words from training model.
  - NLTK library provides excellent support for performing this technique.

* GPT-2
* GPT-3
