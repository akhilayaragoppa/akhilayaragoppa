I"u<p>#Dataset</p>

<p>The <strong>Million Song Dataset</strong> is a freely-available collection of audio features and metadata for a million contemporary popular music tracks. This project uses a <a href="http://millionsongdataset.com/pages/getting-dataset/#subset">subset</a> of this dataset. The data contains 515,345 songs, each described by 90 features, and the year in which the song was released. The years range from 1922 to 2011.</p>

<p>#Exploratory Data Analysis</p>

<p>For simplicity, the number of classes is reduced from 90 to 9 by assigning a decade to each song, instead of year.</p>

<p>A histogram of the count of songs in each decade showed an unevenly distributed data:
<a href="/images/hist-unmodified.png">hist-unmodified</a></p>

<p>Changing the width of the classes resulted in a more evenly distributed graph as shown: (explain why you needed evenly distributed data)
<a href="/images/hist-modified.png">hist-modified</a></p>

<p>#Choosing Features
A feature can be chosen for classification, if it peaks at different places for different classes. For example, I chose ‘Timbre average 1’ as one of the classifiers as it peaked at various regions for Class 1 and Class 2 as shown in the below figure:
<a href="/images/hist-f1-c1-c2.png">hist-f1-c1-c2</a></p>

<p>##Principal Components
A scatter plot of the first two principal components looked something like this:
<a href="/images/scatter-pc.png">scatter-pc</a></p>

<p>Two important observations can be made from this plot:</p>
<ol>
  <li>There is a significant difference in the amount of data between the first few classes and the last couple of classes (it is obscured by pink).</li>
  <li>The colors are all bundled together. Ideally, the different colors must be local to different regions on the plot, so that a more accurate decision can be made when predicting the class of some unknown data.</li>
</ol>

<p>#Bayes’ CLassification
The test data was first classified without reducing any dimensions (i.e., using all 90 features independently). An accuracy of 22% was obtained.</p>

<p>The 90 features were then projected in the direction of the 2 eigenvectors corresponding to the 2 highest eigenvalues. Classifying with the help of the 2 principal components returned an accuracy of around 50%. As this result seemed odd, the procedure was rechecked.
This lead to a conclusion that the dataset has the <strong>curse of dimesionality</strong>. The problem is that when the dimensionality increases, the volume of the space increases so fast that the available data becomes sparse. So, a huge amount of data is required to support analysis in higher dimensions.</p>

<p>After modyfying class sizes to obtain a more evenly distributed data, the prediction accuracy was in fact lower (only about 35%). I do not have a plausible explanation for this. The decision boundary looks as below:
<a href="/images/dec-bound.png">decision-boundary</a></p>
:ET