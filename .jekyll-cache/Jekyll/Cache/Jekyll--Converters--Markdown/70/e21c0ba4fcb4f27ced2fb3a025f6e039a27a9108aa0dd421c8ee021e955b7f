I"t+<h2 id="dataset">Dataset</h2>

<p>The <strong>Million Song Dataset</strong> is a freely-available collection of audio features and metadata for a million contemporary popular music tracks. This project uses a <a href="http://millionsongdataset.com/pages/getting-dataset/#subset">subset</a> of this dataset. The data contains 515,345 songs, each described by 90 features, and the year in which the song was released. The years range from 1922 to 2011.</p>

<h2 id="exploratory-data-analysis">Exploratory Data Analysis</h2>

<p>For simplicity, the number of classes is reduced from 90 to 9 by assigning a decade to each song, instead of year.
A histogram of the count of songs in each decade showed an unevenly distributed data:
<img src="http://localhost:4000/images/hist-unmodified.PNG" alt="Hist-unmodified" /></p>

<p>In an attempt to obtain a more evenly distributed data, the width of the classes was altered, thus resulting in a distribution as shown below:
<img src="/images/hist-modified.PNG" alt="hist-modified" /></p>

<h2 id="choosing-features">Choosing Features</h2>
<p>A feature can be chosen for classification, if it peaks at different places for different classes. For example, I chose ‘Timbre average 1’ as one of the classifiers as it peaked at various regions for Class 1 and Class 2 as shown in the below figure:
<img src="/images/hist-f1-c1-c2.PNG" alt="hist-f1-c1-c2" /></p>

<h3 id="principal-components">Principal Components</h3>
<p>A scatter plot of the first two principal components looked something like this:
<img src="/images/scatter-pc.PNG" alt="scatter-pc" /></p>

<p>Two important observations can be made from this plot:</p>
<ol>
  <li>There is a significant difference in the amount of data between the first few classes and the last couple of classes (it is obscured by pink).</li>
  <li>The colors are all bundled together. Ideally, the different colors must be local to different regions on the plot, so that a more accurate decision can be made when predicting the class of some unknown data.</li>
</ol>

<h2 id="bayes-classification">Bayes’ Classification</h2>
<p>The test data was first classified without reducing any dimensions (i.e., using all 90 features independently). An accuracy of 22% was obtained.</p>

<p>The 90 features were then projected in the direction of the 2 eigenvectors corresponding to the 2 highest eigenvalues. Classifying with the help of the 2 principal components returned an accuracy of around 50%. As this result seemed odd, the procedure was rechecked.
This lead to a conclusion that the dataset has the <strong>curse of dimesionality</strong>. The problem is that when the dimensionality increases, the volume of the space increases so fast that the available data becomes sparse. So, a huge amount of data is required to support analysis in higher dimensions.</p>

<p>After modyfying class sizes to obtain a more evenly distributed data, the prediction accuracy was in fact lower (only about 35%). I do not have a plausible explanation for this. The decision boundary looks as below:
<img src="/images/dec-bound.PNG" alt="decision-boundary" /></p>

<h2 id="implementation">Implementation</h2>
<p>The MATLAB code written to perform the above analysis:</p>
<div class="language-matlab highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">raw</span> <span class="o">=</span> <span class="nb">load</span><span class="p">(</span><span class="s1">'YearPredictionMSD.txt'</span><span class="p">);</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">raw</span><span class="p">(:,</span><span class="mi">2</span><span class="p">:</span><span class="mi">91</span><span class="p">);</span>
<span class="c1">%Separate data for train and test as given in website</span>
<span class="c1">%separate class info from data</span>
<span class="n">trainingData</span> <span class="o">=</span> <span class="n">data</span><span class="p">(</span><span class="mi">1</span><span class="p">:</span><span class="mi">463715</span><span class="p">,:);</span>
<span class="n">class_traingData</span> <span class="o">=</span> <span class="n">raw</span><span class="p">(</span><span class="mi">1</span><span class="p">:</span><span class="mi">463715</span><span class="p">,</span><span class="mi">1</span><span class="p">);</span>
<span class="n">r_class_train</span> <span class="o">=</span> <span class="n">ClassReduce</span><span class="p">(</span><span class="n">class_traingData</span><span class="p">);</span>
<span class="n">testingData</span> <span class="o">=</span> <span class="n">data</span><span class="p">(</span><span class="mi">463716</span><span class="p">:</span><span class="mi">515345</span><span class="p">,:);</span>
<span class="n">class_testingData</span> <span class="o">=</span> <span class="n">raw</span><span class="p">(</span><span class="mi">463716</span><span class="p">:</span><span class="mi">515345</span><span class="p">,</span><span class="mi">1</span><span class="p">);</span>
<span class="n">r_class_test</span> <span class="o">=</span> <span class="n">ClassReduce</span><span class="p">(</span><span class="n">class_testingData</span><span class="p">);</span>

<span class="c1">%histogram of two features</span>
<span class="nb">figure</span><span class="p">,</span><span class="nb">hist</span><span class="p">(</span><span class="n">trainingData</span><span class="p">(:,</span><span class="mi">1</span><span class="p">));</span>
<span class="nb">figure</span><span class="p">,</span><span class="nb">hist</span><span class="p">(</span><span class="n">trainingData</span><span class="p">(:,</span><span class="mi">13</span><span class="p">));</span>

<span class="c1">% find 2 principle components(PC)</span>
<span class="n">coeff</span> <span class="o">=</span> <span class="n">pca</span><span class="p">(</span><span class="n">data</span><span class="p">);</span>
<span class="n">coeff</span> <span class="o">=</span> <span class="n">coeff</span><span class="p">(:,</span><span class="mi">1</span><span class="p">:</span><span class="mi">2</span><span class="p">);</span>
<span class="c1">%figure, mesh(coeff);</span>

<span class="c1">% finding data representation along PC</span>
<span class="n">mu</span> <span class="o">=</span> <span class="nb">mean</span><span class="p">(</span><span class="n">trainingData</span><span class="p">);</span>
<span class="n">lowrep_trainingData</span> <span class="o">=</span> <span class="nb">zeros</span><span class="p">(</span><span class="nb">size</span><span class="p">(</span><span class="n">trainingData</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="mi">2</span><span class="p">);</span>
<span class="k">for</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">1</span><span class="p">:</span><span class="nb">size</span><span class="p">(</span><span class="n">trainingData</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">lowrep_trainingData</span><span class="p">(</span><span class="n">i</span><span class="p">,:)</span> <span class="o">=</span> <span class="p">(</span><span class="n">trainingData</span><span class="p">(</span><span class="n">i</span><span class="p">,:)</span><span class="o">-</span> <span class="n">mu</span><span class="p">)</span> <span class="o">*</span> <span class="n">coeff</span><span class="p">;</span>
<span class="k">end</span>
<span class="n">dim_cov</span> <span class="o">=</span> <span class="nb">cov</span><span class="p">(</span><span class="n">lowrep_trainingData</span><span class="p">);</span>

<span class="n">mu</span> <span class="o">=</span> <span class="nb">mean</span><span class="p">(</span><span class="n">testingData</span><span class="p">);</span>
<span class="n">lowrep_testingData</span> <span class="o">=</span> <span class="nb">zeros</span><span class="p">(</span><span class="nb">size</span><span class="p">(</span><span class="n">testingData</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="mi">2</span><span class="p">);</span>
<span class="k">for</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">1</span><span class="p">:</span><span class="nb">size</span><span class="p">(</span><span class="n">testingData</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">lowrep_testingData</span><span class="p">(</span><span class="n">i</span><span class="p">,:)</span> <span class="o">=</span> <span class="p">(</span><span class="n">testingData</span><span class="p">(</span><span class="n">i</span><span class="p">,:)</span><span class="o">-</span> <span class="n">mu</span><span class="p">)</span> <span class="o">*</span> <span class="n">coeff</span><span class="p">;</span>
<span class="k">end</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">NaiveBayes</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">trainingData</span><span class="p">,</span><span class="n">r_class_train</span><span class="p">);</span>
<span class="n">test_op</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">testingData</span><span class="p">);</span>
<span class="nb">result</span> <span class="o">=</span> <span class="p">((</span><span class="n">test_op</span><span class="o">-</span> <span class="n">r_class_test</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">);</span>
<span class="n">acuracy</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="nb">result</span><span class="p">)/</span><span class="nb">size</span><span class="p">(</span><span class="nb">result</span><span class="p">,</span><span class="mi">1</span><span class="p">);</span>

<span class="n">model2</span> <span class="o">=</span> <span class="n">NaiveBayes</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">lowrep_trainingData</span><span class="p">,</span><span class="n">r_class_train</span><span class="p">);</span>
<span class="n">test_op1</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">model2</span><span class="p">,</span><span class="n">lowrep_testingData</span><span class="p">);</span>
<span class="n">result1</span> <span class="o">=</span> <span class="p">((</span><span class="n">test_op1</span><span class="o">-</span> <span class="n">r_class_test</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">);</span>
<span class="n">acuracy1</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">result1</span><span class="p">)/</span><span class="nb">size</span><span class="p">(</span><span class="n">result1</span><span class="p">,</span><span class="mi">1</span><span class="p">);</span>
</code></pre></div></div>
:ET